{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T08:55:11.392176Z",
     "start_time": "2025-06-23T08:45:46.790251Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "# ------------------------------\n",
    "# DEBUGGING & CONFIGURATION\n",
    "# ------------------------------\n",
    "# IMPORTANT: This forces CUDA operations to be synchronous.\n",
    "# It makes the code slower but provides a correct stack trace if a CUDA error occurs.\n",
    "# This is essential for debugging the \"unknown error\".\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"ViT-H-14\",\n",
    "    \"pretrained\": \"laion2b_s32b_b79k\",\n",
    "    \"csv_train\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/train.csv\",\n",
    "    \"csv_val\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/valid.csv\",\n",
    "    \"csv_test\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/test.csv\",\n",
    "    \"save_path\": \"finetuned_multi_attribute_best.pt\",\n",
    "    \"plot_save_dir\": \"test_results_multi_attribute\",\n",
    "    \"num_plots\": 20,\n",
    "    \"batch_size\": 4, # Adjust based on your VRAM\n",
    "    \"epochs\": 5,\n",
    "    \"lr\": 1e-6, # Lower LR is often better for fine-tuning large models\n",
    "    \"patience\": 3,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    # --- Prompts must match your final inference script ---\n",
    "    \"identity_prompts\": [\n",
    "        \"A face of Soekarno, a male First President (1945-1967) from Indonesia.\",\n",
    "        \"A face of Suharto, a male Second President (1967-1998) from Indonesia.\",\n",
    "        \"A face of Baharuddin Jusuf Habibie, a male Third President (1998-1999) from Indonesia.\",\n",
    "        \"A face of Abdurrahman Wahid, a male Fourth President (1999-2001) from Indonesia.\",\n",
    "        \"A face of Megawati Sukarnoputri, a female Fifth President (2001-2004) from Indonesia.\",\n",
    "        \"A face of Susilo Bambang Yudhoyono, a male Sixth President (2004-2014) from Indonesia.\",\n",
    "        \"A face of Joko Widodo, a male Seventh President (2014-2024) from Indonesia.\",\n",
    "        \"A face of Prabowo Subianto, a male Eight President (2024-Present) from Indonesia.\",\n",
    "        \"A face of Anies Rasyid Baswedan, a male Governor of Jakarta (2017-2022) and Presidential Candidate Election (2024) from Indonesia.\",\n",
    "        \"A face of Ganjar Pranowo, a male Governor of Central Java (2013-2023) and Presidential Candidate Election (2024) from Indonesia.\",\n",
    "        \"A face of Gibran Rakabuming Raka, a male Vice President (2024-2029) from Indonesia.\",\n",
    "        \"A face of Maruf Amin, a male Vice President (2019-2024) from Indonesia.\",\n",
    "        \"A face of Airlangga Hartarto, a male Coordinating Minister of Economic Affairs (2024-2029) from Indonesia.\",\n",
    "        \"A face of Sri Mulyani Indrawati, a female Minister of Finance (2024-2029) from Indonesia.\",\n",
    "        \"A face of Erick Thohir, a male Minister of State Owned Entreprises (2024-2029) from Indonesia.\",\n",
    "        \"A face of Agus Harimurti Yudhoyono, a male Coordinating Minister of Agrarian Affairs and Spatial Planning (2024-2029) and Chairman of Democratic Party from Indonesia.\",\n",
    "        \"A face of Muhaimin Iskandar, a male Coordinating Minister of Social Empowrement (2024-2029) and Chairman of National Awakening Party from Indonesia.\",\n",
    "        \"A face of Mahfud MD, a male Coordinating Minister of Political, Legal, and Security Affairs (2019-2024) from Indonesia.\",\n",
    "        \"A face of Boediono, a male Vice President (2009-2014) from Indonesia.\",\n",
    "        \"A face of Jusuf Kalla, a male Vice President (2004-2009) and Vice President (2014-2019) from Indonesia.\",\n",
    "    ],\n",
    "    \"age_prompts\": [\"a teenager\", \"a young adult\", \"a middle-aged person\", \"a late adult\", \"an elderly person\"],\n",
    "    \"gender_prompts\": [\"a male\", \"a female\"],\n",
    "    \"expression_prompts\": [\n",
    "        \"a person showing anger\", \"a person showing contempt\", \"a person showing disgust\",\n",
    "        \"a person showing happiness\", \"a person showing fear\", \"a person showing sadness\",\n",
    "        \"a person showing surprise\", \"a person with a neutral expression\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Create simplified keys from prompts for mapping ---\n",
    "CONFIG[\"identity_keys\"] = [re.search(r\"of (.*?),\", p).group(1) for p in CONFIG[\"identity_prompts\"]]\n",
    "CONFIG[\"age_keys\"] = [p.replace(\"a \", \"\").replace(\"an \", \"\") for p in CONFIG[\"age_prompts\"]]\n",
    "CONFIG[\"gender_keys\"] = [\"male\", \"female\"]\n",
    "CONFIG[\"expression_keys\"] = [p.split(\" showing \")[-1].split(\" with a \")[-1].replace(\" expression\", \"\") for p in CONFIG[\"expression_prompts\"]]\n",
    "\n",
    "# ------------------------------\n",
    "# Helper Functions\n",
    "# ------------------------------\n",
    "def parse_training_prompt(prompt_text):\n",
    "    \"\"\"Parses various prompt templates to extract name, age_group, and expression.\"\"\"\n",
    "    patterns = [\n",
    "        (r\"(.*?) (?:male|female) named (.*?) with (.*?) expression\\.\", (\"age_group\", \"name\", \"expression\")),\n",
    "        (r\"(.*?) is (.*?) (?:male|female) showing (.*?) face\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"portrait of (.*?), (.*?) (?:male|female) who looks (.*?)\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"face of (.*?), (.*?) (?:male|female), expressing (.*?)\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"(.*?), (.*?) (?:male|female), with (.*?) look\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"the (.*?) face of (.*?), (.*?) (?:male|female)\\.\", (\"expression\", \"name\", \"age_group\")),\n",
    "        (r\"(.*?) looks (.*?), is (.*?) (?:male|female)\\.\", (\"name\", \"expression\", \"age_group\")),\n",
    "        (r\"a photo of (.*?), a (.*?) year old person, with a (.*?) expression\", (\"name\", \"age_group\", \"expression\")),\n",
    "    ]\n",
    "    for pattern, keys in patterns:\n",
    "        match = re.search(pattern, prompt_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return {keys[i]: match.group(i + 1).strip() for i in range(len(keys))}\n",
    "    return None\n",
    "\n",
    "def preprocess_and_cache_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses prompts in a CSV to get ground truth indices and saves a cached version.\n",
    "    If the cached version exists, it loads it directly.\n",
    "    \"\"\"\n",
    "    cache_path = csv_path.replace(\".csv\", \".cached.csv\")\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading preprocessed data from cache: {cache_path}\")\n",
    "        try:\n",
    "            # Also check if cached file is empty\n",
    "            df_cache = pd.read_csv(cache_path)\n",
    "            if df_cache.empty:\n",
    "                print(f\"Warning: Cached file is empty. Reprocessing: {csv_path}\")\n",
    "            else:\n",
    "                return df_cache\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Cached file is empty. Reprocessing: {csv_path}\")\n",
    "\n",
    "    print(f\"Preprocessing and caching data from: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            print(f\"Warning: Original CSV file is empty: {csv_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Original CSV not found at {csv_path}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: No columns to parse from file. It is empty: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    new_data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Preprocessing {os.path.basename(csv_path)}\"):\n",
    "        attrs = parse_training_prompt(row['prompt'])\n",
    "        if attrs:\n",
    "            row_data = {\n",
    "                'filepath': row['filepath'],\n",
    "                'prompt': row['prompt'],\n",
    "                'identity_idx': CONFIG[\"identity_keys\"].index(attrs[\"name\"]) if attrs.get(\"name\") in CONFIG[\"identity_keys\"] else -1,\n",
    "                'gender_idx': 0 if 'male' in row['prompt'].lower() else 1,\n",
    "                'age_idx': CONFIG[\"age_keys\"].index(attrs[\"age_group\"]) if attrs.get(\"age_group\") in CONFIG[\"age_keys\"] else -1,\n",
    "                'expression_idx': CONFIG[\"expression_keys\"].index(attrs[\"expression\"]) if attrs.get(\"expression\") in CONFIG[\"expression_keys\"] else -1\n",
    "            }\n",
    "            new_data.append(row_data)\n",
    "\n",
    "    if not new_data:\n",
    "        print(\"Warning: Preprocessing resulted in an empty dataset. Check your prompts and templates.\")\n",
    "        return None\n",
    "\n",
    "    cached_df = pd.DataFrame(new_data)\n",
    "    cached_df.to_csv(cache_path, index=False)\n",
    "    print(f\"Saved cached data to: {cache_path}\")\n",
    "    return cached_df\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset Class\n",
    "# ------------------------------\n",
    "class FaceAttributeDataset(Dataset):\n",
    "    def __init__(self, df, preprocess):\n",
    "        self.df = df if df is not None else pd.DataFrame()\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row[\"filepath\"]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.preprocess(image)\n",
    "        except FileNotFoundError:\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        gt_indices = torch.tensor([\n",
    "            row[\"identity_idx\"], row[\"gender_idx\"],\n",
    "            row[\"age_idx\"], row[\"expression_idx\"]\n",
    "        ])\n",
    "\n",
    "        return image, gt_indices, image_path, row[\"prompt\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Main Training & Testing Functions\n",
    "# ------------------------------\n",
    "def train_and_validate(model, train_loader, val_loader, all_text_features):\n",
    "    \"\"\"Encapsulates the entire training and validation loop.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1) # Ignore samples where an attribute wasn't found\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    model_saved = False\n",
    "\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        # --- TRAINING ---\n",
    "        model.train()\n",
    "        total_loss_sum = 0\n",
    "        for images, gt_indices, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Train]\"):\n",
    "            images, gt_indices = images.to(CONFIG[\"device\"]), gt_indices.to(CONFIG[\"device\"])\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ all_text_features.T)\n",
    "\n",
    "            offset = 0\n",
    "            identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "            gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "            age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "            expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "            loss_identity = loss_fn(identity_logits, gt_indices[:, 0])\n",
    "            loss_gender = loss_fn(gender_logits, gt_indices[:, 1])\n",
    "            loss_age = loss_fn(age_logits, gt_indices[:, 2])\n",
    "            loss_expression = loss_fn(expression_logits, gt_indices[:, 3])\n",
    "\n",
    "            total_loss = loss_identity + loss_gender + loss_age + loss_expression\n",
    "\n",
    "            optimizer.zero_grad(); total_loss.backward(); optimizer.step()\n",
    "            total_loss_sum += total_loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        total_val_loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for images, gt_indices, _, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Val]\"):\n",
    "                images, gt_indices = images.to(CONFIG[\"device\"]), gt_indices.to(CONFIG[\"device\"])\n",
    "                image_features = model.encode_image(images)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                logits = (100.0 * image_features @ all_text_features.T)\n",
    "\n",
    "                offset = 0\n",
    "                identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "                gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "                age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "                expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "                loss_identity = loss_fn(identity_logits, gt_indices[:, 0])\n",
    "                loss_gender = loss_fn(gender_logits, gt_indices[:, 1])\n",
    "                loss_age = loss_fn(age_logits, gt_indices[:, 2])\n",
    "                loss_expression = loss_fn(expression_logits, gt_indices[:, 3])\n",
    "                total_loss = loss_identity + loss_gender + loss_age + loss_expression\n",
    "                total_val_loss_sum += total_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss_sum / len(val_loader)\n",
    "        print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), CONFIG[\"save_path\"])\n",
    "            print(f\"üéâ Saved best model to {CONFIG['save_path']}\")\n",
    "            patience_counter = 0\n",
    "            model_saved = True # Flag that we have successfully saved a model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚ö†Ô∏è No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "        if patience_counter >= CONFIG[\"patience\"]:\n",
    "            print(\"üõë Early stopping triggered.\"); break\n",
    "\n",
    "    return model_saved\n",
    "\n",
    "\n",
    "def test_and_plot(model, all_text_features, preprocess):\n",
    "    \"\"\"Encapsulates the testing and plotting logic.\"\"\"\n",
    "    print(\"\\n--- Starting Final Testing and Plotting Phase ---\")\n",
    "    model.load_state_dict(torch.load(CONFIG[\"save_path\"])); model.to(CONFIG[\"device\"]).eval()\n",
    "    print(\"Best model loaded.\")\n",
    "\n",
    "    test_df = preprocess_and_cache_csv(CONFIG[\"csv_test\"])\n",
    "    if test_df is None or test_df.empty:\n",
    "        print(f\"Error or empty data in test CSV: {CONFIG['csv_test']}. Halting testing.\")\n",
    "        return\n",
    "\n",
    "    test_dataset = FaceAttributeDataset(test_df, preprocess)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "    print(f\"Found {len(test_dataset)} images in the test set.\")\n",
    "\n",
    "    attribute_correct, total_samples = 0, 0\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, image_paths, texts in tqdm(test_loader, desc=\"[Testing]\"):\n",
    "            images = images.to(CONFIG[\"device\"])\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ all_text_features.T)\n",
    "            offset = 0\n",
    "            identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "            gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "            age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "            expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "            pred_identity_indices = identity_logits.argmax(dim=-1)\n",
    "            pred_gender_indices = gender_logits.argmax(dim=-1)\n",
    "            pred_age_indices = age_logits.argmax(dim=-1)\n",
    "            pred_expression_indices = expression_logits.argmax(dim=-1)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                gt_attrs = parse_training_prompt(texts[i])\n",
    "                if not gt_attrs: continue\n",
    "\n",
    "                pred_attrs = {\n",
    "                    \"name\": CONFIG[\"identity_keys\"][pred_identity_indices[i]],\n",
    "                    \"gender\": CONFIG[\"gender_keys\"][pred_gender_indices[i]],\n",
    "                    \"age_group\": CONFIG[\"age_keys\"][pred_age_indices[i]],\n",
    "                    \"expression\": CONFIG[\"expression_keys\"][pred_expression_indices[i]],\n",
    "                }\n",
    "\n",
    "                is_correct = (gt_attrs.get(\"name\") == pred_attrs.get(\"name\") and\n",
    "                              gt_attrs.get(\"age_group\") == pred_attrs.get(\"age_group\") and\n",
    "                              gt_attrs.get(\"expression\") == pred_attrs.get(\"expression\"))\n",
    "\n",
    "                if is_correct: attribute_correct += 1\n",
    "\n",
    "                all_results.append({\n",
    "                    \"image_path\": image_paths[i], \"gt_attrs\": gt_attrs,\n",
    "                    \"pred_attrs\": pred_attrs, \"is_correct\": is_correct\n",
    "                })\n",
    "                total_samples += 1\n",
    "\n",
    "    if total_samples > 0:\n",
    "        attr_accuracy = (attribute_correct / total_samples) * 100\n",
    "        print(\"\\n--- Test Results ---\")\n",
    "        print(f\"üìä Attribute Accuracy: {attr_accuracy:.2f}% (Correct if Name, Age, and Expression all match)\")\n",
    "        print(\"--------------------\")\n",
    "    else:\n",
    "        print(\"No valid samples were processed in the test set.\")\n",
    "\n",
    "    # --- PLOTTING ---\n",
    "    print(f\"\\n--- Plotting up to {CONFIG['num_plots']} results ---\")\n",
    "    os.makedirs(CONFIG[\"plot_save_dir\"], exist_ok=True)\n",
    "    print(f\"Saving plots to '{os.path.abspath(CONFIG['plot_save_dir'])}'\")\n",
    "\n",
    "    for i, result in enumerate(all_results):\n",
    "        if i >= CONFIG[\"num_plots\"]:\n",
    "            print(f\"Reached plot limit of {CONFIG['num_plots']}.\"); break\n",
    "        try:\n",
    "            img = Image.open(result[\"image_path\"])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find image {result['image_path']} for plotting. Skipping.\"); continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 12))\n",
    "        ax.imshow(img); ax.axis(\"off\")\n",
    "\n",
    "        title = f\"Result {i+1}: {'CORRECT' if result['is_correct'] else 'INCORRECT'}\"\n",
    "        fig.suptitle(title, fontsize=18, color='green' if result['is_correct'] else 'red', y=0.95)\n",
    "\n",
    "        gt_attrs_str = (f\"Ground Truth:\\n\"\n",
    "                        f\"  - Name: {result['gt_attrs'].get('name', 'N/A')}\\n\"\n",
    "                        f\"  - Age: {result['gt_attrs'].get('age_group', 'N/A')}\\n\"\n",
    "                        f\"  - Expression: {result['gt_attrs'].get('expression', 'N/A')}\")\n",
    "\n",
    "        pred_attrs_str = (f\"Prediction:\\n\"\n",
    "                          f\"  - Name: {result['pred_attrs'].get('name', 'N/A')}\\n\"\n",
    "                          f\"  - Age: {result['pred_attrs'].get('age_group', 'N/A')}\\n\"\n",
    "                          f\"  - Expression: {result['pred_attrs'].get('expression', 'N/A')}\")\n",
    "\n",
    "        plt.figtext(0.1, 0.02, gt_attrs_str, ha=\"left\", fontsize=12, wrap=True, va=\"bottom\")\n",
    "        plt.figtext(0.9, 0.02, pred_attrs_str, ha=\"right\", fontsize=12, wrap=True, va=\"bottom\",\n",
    "                    color='green' if result['is_correct'] else 'red')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.1, 1, 0.9])\n",
    "\n",
    "        save_name = f\"result_{i+1}_{'correct' if result['is_correct'] else 'incorrect'}.png\"\n",
    "        plt.savefig(os.path.join(CONFIG[\"plot_save_dir\"], save_name), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"--- Plotting complete ---\")\n",
    "\n",
    "# ==============================\n",
    "#      MAIN EXECUTION BLOCK\n",
    "# ==============================\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {CONFIG['device']}\")\n",
    "    # --- ADDED: Version checking for easier debugging ---\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "    # --- Setup Model and Pre-compute Text Features ---\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        CONFIG[\"model_name\"], pretrained=CONFIG[\"pretrained\"], device=CONFIG[\"device\"]\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(CONFIG[\"model_name\"])\n",
    "    all_prompts = CONFIG[\"identity_prompts\"] + CONFIG[\"gender_prompts\"] + CONFIG[\"age_prompts\"] + CONFIG[\"expression_prompts\"]\n",
    "    all_text_tokens = tokenizer(all_prompts).to(CONFIG[\"device\"])\n",
    "    with torch.no_grad():\n",
    "        all_text_features = model.encode_text(all_text_tokens)\n",
    "        all_text_features = all_text_features / all_text_features.norm(dim=-1, keepdim=True)\n",
    "    print(\"Model and text features loaded.\")\n",
    "\n",
    "    # --- Preprocess Data and Create Dataloaders ---\n",
    "    train_df = preprocess_and_cache_csv(CONFIG[\"csv_train\"])\n",
    "    val_df = preprocess_and_cache_csv(CONFIG[\"csv_val\"])\n",
    "\n",
    "    if train_df is not None and val_df is not None and not train_df.empty and not val_df.empty:\n",
    "        train_dataset = FaceAttributeDataset(train_df, preprocess)\n",
    "        val_dataset = FaceAttributeDataset(val_df, preprocess)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "\n",
    "        # --- Run Training ---\n",
    "        training_successful = train_and_validate(model, train_loader, val_loader, all_text_features)\n",
    "\n",
    "        # --- Run Testing only if Training was Successful ---\n",
    "        if training_successful:\n",
    "            test_and_plot(model, all_text_features, preprocess)\n",
    "        else:\n",
    "            print(\"\\nSkipping testing phase because no model was saved during training.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nTraining/validation datasets are empty or could not be loaded. Skipping training and testing.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch Version: 2.8.0.dev20250507+cu128\n",
      "CUDA Version: 12.8\n",
      "cuDNN Version: 90701\n",
      "Model and text features loaded.\n",
      "Loading preprocessed data from cache: C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/train.cached.csv\n",
      "Loading preprocessed data from cache: C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/valid.cached.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   2%|‚ñè         | 4/175 [09:10<6:32:07, 137.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 401\u001B[39m\n\u001B[32m    398\u001B[39m val_loader = DataLoader(val_dataset, batch_size=CONFIG[\u001B[33m\"\u001B[39m\u001B[33mbatch_size\u001B[39m\u001B[33m\"\u001B[39m], shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers=\u001B[32m0\u001B[39m)\n\u001B[32m    400\u001B[39m \u001B[38;5;66;03m# --- Run Training ---\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m training_successful = \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_text_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[38;5;66;03m# --- Run Testing only if Training was Successful ---\u001B[39;00m\n\u001B[32m    404\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m training_successful:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 195\u001B[39m, in \u001B[36mtrain_and_validate\u001B[39m\u001B[34m(model, train_loader, val_loader, all_text_features)\u001B[39m\n\u001B[32m    192\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, gt_indices, _, _ \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG[\u001B[33m'\u001B[39m\u001B[33mepochs\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m [Train]\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    193\u001B[39m     images, gt_indices = images.to(CONFIG[\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m]), gt_indices.to(CONFIG[\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m195\u001B[39m     image_features = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    196\u001B[39m     image_features = image_features / image_features.norm(dim=-\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    198\u001B[39m     logits = (\u001B[32m100.0\u001B[39m * image_features @ all_text_features.T)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\model.py:279\u001B[39m, in \u001B[36mCLIP.encode_image\u001B[39m\u001B[34m(self, image, normalize)\u001B[39m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mencode_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, normalize: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.normalize(features, dim=-\u001B[32m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m normalize \u001B[38;5;28;01melse\u001B[39;00m features\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:827\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    825\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor):\n\u001B[32m    826\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._embeds(x)\n\u001B[32m--> \u001B[39m\u001B[32m827\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    828\u001B[39m     pooled, tokens = \u001B[38;5;28mself\u001B[39m._pool(x)\n\u001B[32m    830\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.proj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:504\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    502\u001B[39m         x = checkpoint(r, x, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, attn_mask, use_reentrant=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    503\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m504\u001B[39m         x = \u001B[43mr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batch_first:\n\u001B[32m    507\u001B[39m     x = x.transpose(\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)    \u001B[38;5;66;03m# LND -> NLD\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:267\u001B[39m, in \u001B[36mResidualAttentionBlock.forward\u001B[39m\u001B[34m(self, q_x, k_x, v_x, attn_mask)\u001B[39m\n\u001B[32m    265\u001B[39m v_x = \u001B[38;5;28mself\u001B[39m.ln_1_kv(v_x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mln_1_kv\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m v_x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    266\u001B[39m x = q_x + \u001B[38;5;28mself\u001B[39m.ls_1(\u001B[38;5;28mself\u001B[39m.attention(q_x=\u001B[38;5;28mself\u001B[39m.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001B[32m--> \u001B[39m\u001B[32m267\u001B[39m x = x + \u001B[38;5;28mself\u001B[39m.ls_2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mln_2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    268\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:245\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    244\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    246\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcaf5fa24a6fd405"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

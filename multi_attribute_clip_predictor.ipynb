{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os\n",
    "\n",
    "# Set your model configuration\n",
    "MODEL_NAME = \"ViT-B-32\"\n",
    "CHECKPOINT_PATH = \"your_finetuned_clip_checkpoint.pt\"  # <-- replace with your fine-tuned model path\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and preprocessing\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=CHECKPOINT_PATH)\n",
    "model = model.to(DEVICE).eval()\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "\n",
    "# ----- Define Prompt Sets -----\n",
    "IDENTITY_LIST = [\"Daniel\", \"Yuki\", \"Alex\", \"Emma\", \"John\"]\n",
    "AGE_PROMPTS = [\n",
    "    \"A child\",\n",
    "    \"A teenager\",\n",
    "    \"A 25-year-old adult\",\n",
    "    \"A 40-year-old adult\",\n",
    "    \"A 60-year-old senior\"\n",
    "]\n",
    "GENDER_PROMPTS = [\n",
    "    \"A man\",\n",
    "    \"A woman\",\n",
    "    \"A non-binary person\"\n",
    "]\n",
    "EXPRESSION_PROMPTS = [\n",
    "    \"A person with a happy expression\",\n",
    "    \"A person with a sad expression\",\n",
    "    \"A person with an angry expression\",\n",
    "    \"A person with a surprised expression\",\n",
    "    \"A person with a neutral expression\"\n",
    "]\n",
    "\n",
    "IDENTITY_PROMPTS = [f\"A person named {name}\" for name in IDENTITY_LIST]\n",
    "\n",
    "# ----- Helper Functions -----\n",
    "def encode_text_prompts(prompts, tokenizer, model):\n",
    "    tokens = tokenizer(prompts).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "def encode_image(image_path, preprocess, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features\n",
    "\n",
    "def predict_attribute(image_features, text_features, prompt_list, top_k=1):\n",
    "    similarities = (image_features @ text_features.T).squeeze(0)\n",
    "    topk = similarities.topk(top_k)\n",
    "    return [(prompt_list[i], similarities[i].item()) for i in topk.indices]\n",
    "\n",
    "# ----- Main Inference Function -----\n",
    "def predict_face_attributes(image_path):\n",
    "    print(f\"\\n🔍 Predicting attributes for: {image_path}\")\n",
    "\n",
    "    # Encode image\n",
    "    image_features = encode_image(image_path, preprocess, model)\n",
    "\n",
    "    # Encode text prompts\n",
    "    identity_feats = encode_text_prompts(IDENTITY_PROMPTS, tokenizer, model)\n",
    "    age_feats = encode_text_prompts(AGE_PROMPTS, tokenizer, model)\n",
    "    gender_feats = encode_text_prompts(GENDER_PROMPTS, tokenizer, model)\n",
    "    expr_feats = encode_text_prompts(EXPRESSION_PROMPTS, tokenizer, model)\n",
    "\n",
    "    # Predict\n",
    "    identity_pred = predict_attribute(image_features, identity_feats, IDENTITY_PROMPTS)[0]\n",
    "    age_pred = predict_attribute(image_features, age_feats, AGE_PROMPTS)[0]\n",
    "    gender_pred = predict_attribute(image_features, gender_feats, GENDER_PROMPTS)[0]\n",
    "    expression_pred = predict_attribute(image_features, expr_feats, EXPRESSION_PROMPTS)[0]\n",
    "\n",
    "    return {\n",
    "        \"identity\": identity_pred[0],\n",
    "        \"identity_score\": identity_pred[1],\n",
    "        \"age\": age_pred[0],\n",
    "        \"age_score\": age_pred[1],\n",
    "        \"gender\": gender_pred[0],\n",
    "        \"gender_score\": gender_pred[1],\n",
    "        \"expression\": expression_pred[0],\n",
    "        \"expression_score\": expression_pred[1]\n",
    "    }\n",
    "\n",
    "# ----- CLI Example -----\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Multi-Attribute Face Predictor using OpenCLIP\")\n",
    "    parser.add_argument(\"image_path\", type=str, help=\"Path to face image\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.image_path):\n",
    "        print(\"❌ Image file does not exist.\")\n",
    "        exit(1)\n",
    "\n",
    "    results = predict_face_attributes(args.image_path)\n",
    "    print(\"\\n🧠 Prediction Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k:>18}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

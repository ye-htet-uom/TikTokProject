{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T09:40:01.363188Z",
     "start_time": "2025-06-23T09:38:31.314713Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "# ------------------------------\n",
    "# DEBUGGING & CONFIGURATION\n",
    "# ------------------------------\n",
    "# This forces CUDA operations to be synchronous for accurate error reporting.\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"ViT-B-16\",\n",
    "    \"pretrained\": \"datacomp_xl_s13b_b90k\",\n",
    "    \"csv_train\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/train.csv\",\n",
    "    \"csv_val\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/valid.csv\",\n",
    "    \"csv_test\": \"C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/test.csv\",\n",
    "    \"save_path\": \"finetuned_multi_attribute_final.pt\",\n",
    "    \"plot_save_dir\": \"test_results_multi_attribute_final\",\n",
    "    \"num_plots\": 20,\n",
    "    \"batch_size\": 4, # Start small for large models to avoid CUDA OOM errors\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 1e-6,\n",
    "    \"patience\": 3,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    \"identity_prompts\": [\n",
    "        \"A photo of Soekarno.\", \"A photo of Suharto.\", \"A photo of Baharuddin Jusuf Habibie.\",\n",
    "        \"A photo of Abdurrahman Wahid.\", \"A photo of Megawati Sukarnoputri.\",\n",
    "        \"A photo of Susilo Bambang Yudhoyono.\", \"A photo of Joko Widodo.\",\n",
    "        \"A photo of Prabowo Subianto.\", \"A photo of Anies Rasyid Baswedan.\",\n",
    "        \"A photo of Ganjar Pranowo.\", \"A photo of Gibran Rakabuming Raka.\",\n",
    "        \"A photo of Maruf Amin.\", \"A photo of Airlangga Hartarto.\",\n",
    "        \"A photo of Sri Mulyani Indrawati.\", \"A photo of Erick Thohir.\",\n",
    "        \"A photo of Agus Harimurti Yudhoyono.\", \"A photo of Muhaimin Iskandar.\",\n",
    "        \"A photo of Mahfud MD.\", \"A photo of Boediono\", \"A photo of Jusuf Kalla\"\n",
    "    ],\n",
    "    \"age_prompts\": [\"a photo of a teenager.\", \"a photo of a young adult.\", \"a photo of a middle-aged person.\", \"a photo of a late adult.\", \"a photo of an elderly person.\"],\n",
    "    \"gender_prompts\": [\"a photo of a male person.\", \"a photo of a female person.\"],\n",
    "    \"expression_prompts\": [\n",
    "        \"a photo of a person with an anger expression.\", \"a photo of a person with a contempt expression.\",\n",
    "        \"a photo of a person with a disgust expression.\", \"a photo of a person with a happiness expression.\",\n",
    "        \"a photo of a person with a fear expression.\", \"a photo of a person with a sadness expression.\",\n",
    "        \"a photo of a person with a surprise expression.\", \"a photo of a person with a neutral expression.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Create simplified keys from prompts for mapping ---\n",
    "CONFIG[\"identity_keys\"] = [p.replace(\"A photo of \", \"\").replace(\".\", \"\") for p in CONFIG[\"identity_prompts\"]]\n",
    "CONFIG[\"age_keys\"] = [p.replace(\"a photo of a \", \"\").replace(\"an \", \"\").replace(\".\", \"\") for p in CONFIG[\"age_prompts\"]]\n",
    "CONFIG[\"gender_keys\"] = [\"male\", \"female\"]\n",
    "CONFIG[\"expression_keys\"] = [p.split(\" with a \")[-1].replace(\" expression.\", \"\").replace(\"an \", \"\") for p in CONFIG[\"expression_prompts\"]]\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Helper Functions\n",
    "# ------------------------------\n",
    "def parse_training_prompt(prompt_text):\n",
    "    \"\"\"Parses various prompt templates to extract all attributes.\"\"\"\n",
    "    patterns = [\n",
    "        (r\"(.*?) (?:male|female) named (.*?) with (.*?) expression\\.\", (\"age_group\", \"name\", \"expression\")),\n",
    "        (r\"(.*?) is (.*?) (?:male|female) showing (.*?) face\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"portrait of (.*?), (.*?) (?:male|female) who looks (.*?)\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"face of (.*?), (.*?) (?:male|female), expressing (.*?)\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"(.*?), (.*?) (?:male|female), with (.*?) look\\.\", (\"name\", \"age_group\", \"expression\")),\n",
    "        (r\"the (.*?) face of (.*?), (.*?) (?:male|female)\\.\", (\"expression\", \"name\", \"age_group\")),\n",
    "        (r\"(.*?) looks (.*?), is (.*?) (?:male|female)\\.\", (\"name\", \"expression\", \"age_group\")),\n",
    "        (r\"a photo of (.*?), a (.*?) year old person, with a (.*?) expression\", (\"name\", \"age_group\", \"expression\")),\n",
    "    ]\n",
    "    for pattern, keys in patterns:\n",
    "        match = re.search(pattern, prompt_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            attrs = {keys[i]: match.group(i + 1).strip() for i in range(len(keys))}\n",
    "            # Add gender based on keyword search\n",
    "            attrs['gender'] = 'male' if 'male' in prompt_text.lower() else 'female'\n",
    "            return attrs\n",
    "    return None\n",
    "\n",
    "def preprocess_and_cache_csv(csv_path):\n",
    "    \"\"\"Parses all attributes and saves a single cached version.\"\"\"\n",
    "    cache_path = csv_path.replace(\".csv\", \".multi_attribute_cached.csv\")\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading preprocessed data from cache: {cache_path}\")\n",
    "        try:\n",
    "            return pd.read_csv(cache_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Cached file is empty. Reprocessing.\")\n",
    "\n",
    "    print(f\"Preprocessing and caching data from: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            print(f\"Warning: Original CSV file is empty: {csv_path}\")\n",
    "            return None\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(f\"Error or empty file at {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    new_data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Preprocessing {os.path.basename(csv_path)}\"):\n",
    "        attrs = parse_training_prompt(row['prompt'])\n",
    "        if attrs:\n",
    "            row_data = {\n",
    "                'filepath': row['filepath'],\n",
    "                'prompt': row['prompt'],\n",
    "                'identity_idx': CONFIG[\"identity_keys\"].index(attrs[\"name\"]) if attrs.get(\"name\") in CONFIG[\"identity_keys\"] else -1,\n",
    "                'gender_idx': CONFIG[\"gender_keys\"].index(attrs[\"gender\"]) if attrs.get(\"gender\") in CONFIG[\"gender_keys\"] else -1,\n",
    "                'age_idx': CONFIG[\"age_keys\"].index(attrs[\"age_group\"]) if attrs.get(\"age_group\") in CONFIG[\"age_keys\"] else -1,\n",
    "                'expression_idx': CONFIG[\"expression_keys\"].index(attrs[\"expression\"]) if attrs.get(\"expression\") in CONFIG[\"expression_keys\"] else -1\n",
    "            }\n",
    "            new_data.append(row_data)\n",
    "\n",
    "    if not new_data:\n",
    "        print(\"Warning: Preprocessing resulted in an empty dataset. Check prompt templates and keys.\")\n",
    "        return None\n",
    "\n",
    "    cached_df = pd.DataFrame(new_data)\n",
    "    cached_df.to_csv(cache_path, index=False)\n",
    "    return cached_df\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset Class\n",
    "# ------------------------------\n",
    "class MultiAttributeDataset(Dataset):\n",
    "    def __init__(self, df, preprocess):\n",
    "        self.df = df if df is not None else pd.DataFrame()\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row[\"filepath\"]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.preprocess(image)\n",
    "        except FileNotFoundError:\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        gt_indices = torch.tensor([\n",
    "            row[\"identity_idx\"], row[\"gender_idx\"],\n",
    "            row[\"age_idx\"], row[\"expression_idx\"]\n",
    "        ])\n",
    "\n",
    "        return image, gt_indices, image_path\n",
    "\n",
    "# ------------------------------\n",
    "# Main Training & Testing Functions\n",
    "# ------------------------------\n",
    "def train_and_validate(model, train_loader, val_loader, all_text_features):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    # --- ADDED: ignore_index=-1 tells the loss function to skip samples that couldn't be parsed.\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    model_saved = False\n",
    "\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss_sum = 0\n",
    "        for images, gt_indices, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Train]\"):\n",
    "            images, gt_indices = images.to(CONFIG[\"device\"]), gt_indices.to(CONFIG[\"device\"])\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ all_text_features.T)\n",
    "\n",
    "            offset = 0\n",
    "            identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "            gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "            age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "            expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "            loss_identity = loss_fn(identity_logits, gt_indices[:, 0])\n",
    "            loss_gender = loss_fn(gender_logits, gt_indices[:, 1])\n",
    "            loss_age = loss_fn(age_logits, gt_indices[:, 2])\n",
    "            loss_expression = loss_fn(expression_logits, gt_indices[:, 3])\n",
    "\n",
    "            total_loss = loss_identity + loss_gender + loss_age + loss_expression\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # --- ADDED: Gradient Clipping to prevent exploding gradients and NaN loss ---\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss_sum += total_loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for images, gt_indices, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Val]\"):\n",
    "                images, gt_indices = images.to(CONFIG[\"device\"]), gt_indices.to(CONFIG[\"device\"])\n",
    "                image_features = model.encode_image(images)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                logits = (100.0 * image_features @ all_text_features.T)\n",
    "\n",
    "                offset = 0\n",
    "                identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "                gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "                age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "                expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "                loss_identity = loss_fn(identity_logits, gt_indices[:, 0])\n",
    "                loss_gender = loss_fn(gender_logits, gt_indices[:, 1])\n",
    "                loss_age = loss_fn(age_logits, gt_indices[:, 2])\n",
    "                loss_expression = loss_fn(expression_logits, gt_indices[:, 3])\n",
    "                total_loss = loss_identity + loss_gender + loss_age + loss_expression\n",
    "                total_val_loss_sum += total_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss_sum / len(val_loader)\n",
    "        print(f\"‚úÖ Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), CONFIG[\"save_path\"])\n",
    "            print(f\"üéâ Saved best model to {CONFIG['save_path']}\")\n",
    "            patience_counter = 0\n",
    "            model_saved = True\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚ö†Ô∏è No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "        if patience_counter >= CONFIG[\"patience\"]:\n",
    "            print(\"üõë Early stopping triggered.\"); break\n",
    "\n",
    "    return model_saved\n",
    "\n",
    "def test_and_plot(model, all_text_features, preprocess):\n",
    "    print(\"\\n--- Starting Final Testing and Plotting Phase ---\")\n",
    "    model.load_state_dict(torch.load(CONFIG[\"save_path\"])); model.to(CONFIG[\"device\"]).eval()\n",
    "    print(\"Best model loaded.\")\n",
    "\n",
    "    test_df = preprocess_and_cache_csv(CONFIG[\"csv_test\"])\n",
    "    if test_df is None or test_df.empty:\n",
    "        print(f\"Error or empty data in test CSV. Halting testing.\")\n",
    "        return\n",
    "\n",
    "    test_dataset = MultiAttributeDataset(test_df, preprocess)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    attribute_correct, total_samples = 0, 0\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_indices, image_paths in tqdm(test_loader, desc=\"[Testing]\"):\n",
    "            images = images.to(CONFIG[\"device\"])\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ all_text_features.T)\n",
    "            offset = 0\n",
    "            identity_logits = logits[:, offset:offset+len(CONFIG[\"identity_prompts\"])]; offset += len(CONFIG[\"identity_prompts\"])\n",
    "            gender_logits = logits[:, offset:offset+len(CONFIG[\"gender_prompts\"])]; offset += len(CONFIG[\"gender_prompts\"])\n",
    "            age_logits = logits[:, offset:offset+len(CONFIG[\"age_prompts\"])]; offset += len(CONFIG[\"age_prompts\"])\n",
    "            expression_logits = logits[:, offset:offset+len(CONFIG[\"expression_prompts\"])]\n",
    "\n",
    "            pred_identity_indices = identity_logits.argmax(dim=-1)\n",
    "            pred_gender_indices = gender_logits.argmax(dim=-1)\n",
    "            pred_age_indices = age_logits.argmax(dim=-1)\n",
    "            pred_expression_indices = expression_logits.argmax(dim=-1)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                gt_attrs = {\n",
    "                    \"name\": CONFIG[\"identity_keys\"][gt_indices[i, 0]],\n",
    "                    \"gender\": CONFIG[\"gender_keys\"][gt_indices[i, 1]],\n",
    "                    \"age_group\": CONFIG[\"age_keys\"][gt_indices[i, 2]],\n",
    "                    \"expression\": CONFIG[\"expression_keys\"][gt_indices[i, 3]],\n",
    "                }\n",
    "\n",
    "                pred_attrs = {\n",
    "                    \"name\": CONFIG[\"identity_keys\"][pred_identity_indices[i]],\n",
    "                    \"gender\": CONFIG[\"gender_keys\"][pred_gender_indices[i]],\n",
    "                    \"age_group\": CONFIG[\"age_keys\"][pred_age_indices[i]],\n",
    "                    \"expression\": CONFIG[\"expression_keys\"][pred_expression_indices[i]],\n",
    "                }\n",
    "\n",
    "                is_correct = (gt_attrs[\"name\"] == pred_attrs[\"name\"] and\n",
    "                              gt_attrs[\"age_group\"] == pred_attrs[\"age_group\"] and\n",
    "                              gt_attrs[\"expression\"] == pred_attrs[\"expression\"])\n",
    "\n",
    "                if is_correct: attribute_correct += 1\n",
    "\n",
    "                all_results.append({\n",
    "                    \"image_path\": image_paths[i], \"gt_attrs\": gt_attrs,\n",
    "                    \"pred_attrs\": pred_attrs, \"is_correct\": is_correct\n",
    "                })\n",
    "                total_samples += 1\n",
    "\n",
    "    if total_samples > 0:\n",
    "        attr_accuracy = (attribute_correct / total_samples) * 100\n",
    "        print(\"\\n--- Test Results ---\")\n",
    "        print(f\"üìä Attribute Accuracy: {attr_accuracy:.2f}% (Correct if Name, Age, and Expression all match)\")\n",
    "    else:\n",
    "        print(\"No valid samples were processed in the test set.\")\n",
    "\n",
    "    print(f\"\\n--- Plotting up to {CONFIG['num_plots']} results ---\")\n",
    "    os.makedirs(CONFIG[\"plot_save_dir\"], exist_ok=True)\n",
    "\n",
    "    for i, result in enumerate(all_results):\n",
    "        if i >= CONFIG[\"num_plots\"]: break\n",
    "        try:\n",
    "            img = Image.open(result[\"image_path\"])\n",
    "        except FileNotFoundError: continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 12))\n",
    "        ax.imshow(img); ax.axis(\"off\")\n",
    "\n",
    "        title = f\"Result {i+1}: {'CORRECT' if result['is_correct'] else 'INCORRECT'}\"\n",
    "        fig.suptitle(title, fontsize=18, color='green' if result['is_correct'] else 'red', y=0.95)\n",
    "\n",
    "        gt_attrs_str = (f\"Ground Truth:\\n\"\n",
    "                        f\"  - Name: {result['gt_attrs'].get('name', 'N/A')}\\n\"\n",
    "                        f\"  - Age: {result['gt_attrs'].get('age_group', 'N/A')}\\n\"\n",
    "                        f\"  - Expression: {result['gt_attrs'].get('expression', 'N/A')}\")\n",
    "\n",
    "        pred_attrs_str = (f\"Prediction:\\n\"\n",
    "                          f\"  - Name: {result['pred_attrs'].get('name', 'N/A')}\\n\"\n",
    "                          f\"  - Age: {result['pred_attrs'].get('age_group', 'N/A')}\\n\"\n",
    "                          f\"  - Expression: {result['pred_attrs'].get('expression', 'N/A')}\")\n",
    "\n",
    "        plt.figtext(0.1, 0.02, gt_attrs_str, ha=\"left\", fontsize=12, wrap=True, va=\"bottom\")\n",
    "        plt.figtext(0.9, 0.02, pred_attrs_str, ha=\"right\", fontsize=12, wrap=True, va=\"bottom\",\n",
    "                    color='green' if result['is_correct'] else 'red')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.1, 1, 0.9])\n",
    "\n",
    "        save_name = f\"result_{i+1}_{'correct' if result['is_correct'] else 'incorrect'}.png\"\n",
    "        plt.savefig(os.path.join(CONFIG[\"plot_save_dir\"], save_name), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"--- Plotting complete ---\")\n",
    "\n",
    "# ==============================\n",
    "#      MAIN EXECUTION BLOCK\n",
    "# ==============================\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {CONFIG['device']}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"PyTorch Version: {torch.__version__}, CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        CONFIG[\"model_name\"], pretrained=CONFIG[\"pretrained\"], device=CONFIG[\"device\"]\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(CONFIG[\"model_name\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_prompts = CONFIG[\"identity_prompts\"] + CONFIG[\"gender_prompts\"] + CONFIG[\"age_prompts\"] + CONFIG[\"expression_prompts\"]\n",
    "        all_text_tokens = tokenizer(all_prompts).to(CONFIG[\"device\"])\n",
    "        all_text_features = model.encode_text(all_text_tokens)\n",
    "        all_text_features = all_text_features / all_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    train_df = preprocess_and_cache_csv(CONFIG[\"csv_train\"])\n",
    "    val_df = preprocess_and_cache_csv(CONFIG[\"csv_val\"])\n",
    "\n",
    "    if train_df is not None and val_df is not None and not train_df.empty and not val_df.empty:\n",
    "        train_dataset = MultiAttributeDataset(train_df, preprocess)\n",
    "        val_dataset = MultiAttributeDataset(val_df, preprocess)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "        training_successful = train_and_validate(model, train_loader, val_loader, all_text_features)\n",
    "\n",
    "        if training_successful:\n",
    "            test_and_plot(model, all_text_features, preprocess)\n",
    "        else:\n",
    "            print(\"\\nSkipping testing phase: No model was saved during training.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping training: Training/validation datasets are empty or could not be loaded.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch Version: 2.8.0.dev20250507+cu128, CUDA Version: 12.8\n",
      "Loading preprocessed data from cache: C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/train.multi_attribute_cached.csv\n",
      "Loading preprocessed data from cache: C:/Users/yehte/Downloads/Ye Htet/Projects/TikTok/Annotation/fine-tune/valid.multi_attribute_cached.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   2%|‚ñè         | 4/175 [01:04<45:42, 16.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 373\u001B[39m\n\u001B[32m    370\u001B[39m train_loader = DataLoader(train_dataset, batch_size=CONFIG[\u001B[33m\"\u001B[39m\u001B[33mbatch_size\u001B[39m\u001B[33m\"\u001B[39m], shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    371\u001B[39m val_loader = DataLoader(val_dataset, batch_size=CONFIG[\u001B[33m\"\u001B[39m\u001B[33mbatch_size\u001B[39m\u001B[33m\"\u001B[39m], shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m training_successful = \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_text_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m training_successful:\n\u001B[32m    376\u001B[39m     test_and_plot(model, all_text_features, preprocess)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 172\u001B[39m, in \u001B[36mtrain_and_validate\u001B[39m\u001B[34m(model, train_loader, val_loader, all_text_features)\u001B[39m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, gt_indices, _ \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG[\u001B[33m'\u001B[39m\u001B[33mepochs\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m [Train]\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    170\u001B[39m     images, gt_indices = images.to(CONFIG[\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m]), gt_indices.to(CONFIG[\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m     image_features = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    173\u001B[39m     image_features = image_features / image_features.norm(dim=-\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    175\u001B[39m     logits = (\u001B[32m100.0\u001B[39m * image_features @ all_text_features.T)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\model.py:279\u001B[39m, in \u001B[36mCLIP.encode_image\u001B[39m\u001B[34m(self, image, normalize)\u001B[39m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mencode_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, normalize: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.normalize(features, dim=-\u001B[32m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m normalize \u001B[38;5;28;01melse\u001B[39;00m features\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:827\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    825\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor):\n\u001B[32m    826\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._embeds(x)\n\u001B[32m--> \u001B[39m\u001B[32m827\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    828\u001B[39m     pooled, tokens = \u001B[38;5;28mself\u001B[39m._pool(x)\n\u001B[32m    830\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.proj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:504\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    502\u001B[39m         x = checkpoint(r, x, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, attn_mask, use_reentrant=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    503\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m504\u001B[39m         x = \u001B[43mr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batch_first:\n\u001B[32m    507\u001B[39m     x = x.transpose(\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)    \u001B[38;5;66;03m# LND -> NLD\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\open_clip\\transformer.py:267\u001B[39m, in \u001B[36mResidualAttentionBlock.forward\u001B[39m\u001B[34m(self, q_x, k_x, v_x, attn_mask)\u001B[39m\n\u001B[32m    265\u001B[39m v_x = \u001B[38;5;28mself\u001B[39m.ln_1_kv(v_x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mln_1_kv\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m v_x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    266\u001B[39m x = q_x + \u001B[38;5;28mself\u001B[39m.ls_1(\u001B[38;5;28mself\u001B[39m.attention(q_x=\u001B[38;5;28mself\u001B[39m.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001B[32m--> \u001B[39m\u001B[32m267\u001B[39m x = x + \u001B[38;5;28mself\u001B[39m.ls_2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mln_2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    268\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:245\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    244\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    246\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1753\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1761\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1762\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1763\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1764\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1765\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1768\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1769\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TikTokProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
